{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works for all types of entries\n",
    "def create_sentences(segments_data, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "  # Combine the non-sentences together\n",
    "  sentences = []\n",
    "\n",
    "  is_new_sentence = True\n",
    "  sentence_length = 0\n",
    "  sentence_num = 0\n",
    "  sentence_segments = []\n",
    "\n",
    "  for i in range(len(segments_data)):\n",
    "    if is_new_sentence == True:\n",
    "      is_new_sentence = False\n",
    "    # Append the segment\n",
    "    sentence_segments.append(segments_data[i]['text'])\n",
    "    segment_words = segments_data[i]['text'].split(' ')\n",
    "    sentence_length += len(segment_words)\n",
    "    \n",
    "    # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "    # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "    if (sentence_length >= MIN_WORDS and segments_data[i]['text'][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "      sentence = ' '.join(sentence_segments)\n",
    "      sentences.append({\n",
    "        'sentence_num': sentence_num,\n",
    "        'text': sentence,\n",
    "        'sentence_length': sentence_length,\n",
    "        'start_chunk_id': i - len(sentence_segments) + 1,\n",
    "        'end_chunk_id': i,\n",
    "        'start_time': segments_data[i - len(sentence_segments) + 1].get('start_time', None),\n",
    "        'end_time': segments_data[i].get('end_time', None)\n",
    "      })\n",
    "      # Reset\n",
    "      is_new_sentence = True\n",
    "      sentence_length = 0\n",
    "      sentence_segments = []\n",
    "      sentence_num += 1\n",
    "\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 14):\n",
    "  with open(f'{i}.json') as f:\n",
    "    entry_data = json.load(f)\n",
    "      \n",
    "  chunks_data = entry_data['chunks']\n",
    "  chunks = [ chunk['text'] for chunk in chunks_data ]\n",
    "\n",
    "  chunks_df = pd.DataFrame(chunks_data)\n",
    "  chunks_df['emotions'] = chunks_df['emotions'].apply(lambda x: np.array(x))\n",
    "  chunks_df['index'] = chunks_df.index\n",
    "  if 'start_time' in chunks_df.columns and 'end_time' in chunks_df.columns:\n",
    "    chunks_df['duration'] = chunks_df['end_time'] - chunks_df['start_time']\n",
    "  else:\n",
    "    # get number of words of text\n",
    "    chunks_df['duration'] = chunks_df['text'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "  # Combines chunks (some are too short to be meaningful) into sentences of 20-30 words\n",
    "  sentences = create_sentences(chunks_data, MIN_WORDS=20, MAX_WORDS=30)\n",
    "  sentences_df = pd.DataFrame(sentences)\n",
    "\n",
    "  # For each sentence in sentences_df, create an emotions column, \n",
    "  # which is a weighted average of the emotions of the chunks that make up the sentence (start_chunk_id to end_chunk_id inclusive)\n",
    "  # where the weights are the duration\n",
    "\n",
    "  sentences_df['emotions'] = sentences_df.apply( lambda row: np.average(chunks_df.loc[row['start_chunk_id']:row['end_chunk_id']]['emotions'],\n",
    "                                                                        weights=chunks_df.loc[row['start_chunk_id']:row['end_chunk_id']]['duration']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence_num': 0,\n",
       "  'text': \"Yeah. Happy New year. Guess, I haven't driven million in a wall. Yeah we've got like ten more days two school starts.\",\n",
       "  'sentence_length': 22,\n",
       "  'start_chunk_id': 0,\n",
       "  'end_chunk_id': 1,\n",
       "  'start_time': 0.6982051,\n",
       "  'end_time': 6.903538},\n",
       " {'sentence_num': 1,\n",
       "  'text': \"I don't know how to feel about that. I think it's nice to be at home but I I miss... I'm miss my friends and I miss Kayla and Yeah. I...\",\n",
       "  'sentence_length': 31,\n",
       "  'start_chunk_id': 2,\n",
       "  'end_chunk_id': 5,\n",
       "  'start_time': 8.454273,\n",
       "  'end_time': 18.39785},\n",
       " {'sentence_num': 2,\n",
       "  'text': 'Yeah. I think it just be nice to see K in. I, Acknowledge is nice you get your own space. You get your own room nice to hang out. And,',\n",
       "  'sentence_length': 30,\n",
       "  'start_chunk_id': 6,\n",
       "  'end_chunk_id': 8,\n",
       "  'start_time': 18.736076,\n",
       "  'end_time': 28.313456},\n",
       " {'sentence_num': 3,\n",
       "  'text': \"yeah. I maybe spending too much time at home can also get a little bit dull. But it's it's mine. It's been nice. I really like I really enjoyed a Europe trip. It was it was amazing\",\n",
       "  'sentence_length': 37,\n",
       "  'start_chunk_id': 9,\n",
       "  'end_chunk_id': 11,\n",
       "  'start_time': 29.132057,\n",
       "  'end_time': 39.24},\n",
       " {'sentence_num': 4,\n",
       "  'text': 'The such sounds is just something else to see. But... Yeah. Yeah. I I guess I would just enjoy the rest of the time here.',\n",
       "  'sentence_length': 25,\n",
       "  'start_chunk_id': 12,\n",
       "  'end_chunk_id': 15,\n",
       "  'start_time': 39.860004,\n",
       "  'end_time': 50.089497},\n",
       " {'sentence_num': 5,\n",
       "  'text': 'Yeah. Some of the things I really enjoyed in Europe I think Paris was beautiful. I love the... I love the the pastries in Paris actually...',\n",
       "  'sentence_length': 26,\n",
       "  'start_chunk_id': 16,\n",
       "  'end_chunk_id': 19,\n",
       "  'start_time': 50.708008,\n",
       "  'end_time': 60.5328},\n",
       " {'sentence_num': 6,\n",
       "  'text': \"Yeah, the the French the french to do do mean forget... Yeah. But I hope and... Yeah. I think it's just the architecture in Europe is just different and just\",\n",
       "  'sentence_length': 30,\n",
       "  'start_chunk_id': 20,\n",
       "  'end_chunk_id': 25,\n",
       "  'start_time': 61.91088,\n",
       "  'end_time': 77.00038}]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Hume Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json file\n",
    "with open('../embeddings/audio1.json') as f:\n",
    "  data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = data[0]['results']['predictions'][0]['models']['language']['grouped_predictions'][0]['predictions'][0]['emotions']\n",
    "emotion_names = [e['name'] for e in emotions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_names = emotion_names[:48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Admiration',\n",
       " 'Adoration',\n",
       " 'Aesthetic Appreciation',\n",
       " 'Amusement',\n",
       " 'Anger',\n",
       " 'Annoyance',\n",
       " 'Anxiety',\n",
       " 'Awe',\n",
       " 'Awkwardness',\n",
       " 'Boredom',\n",
       " 'Calmness',\n",
       " 'Concentration',\n",
       " 'Confusion',\n",
       " 'Contemplation',\n",
       " 'Contempt',\n",
       " 'Contentment',\n",
       " 'Craving',\n",
       " 'Desire',\n",
       " 'Determination',\n",
       " 'Disappointment',\n",
       " 'Disapproval',\n",
       " 'Disgust',\n",
       " 'Distress',\n",
       " 'Doubt',\n",
       " 'Ecstasy',\n",
       " 'Embarrassment',\n",
       " 'Empathic Pain',\n",
       " 'Enthusiasm',\n",
       " 'Entrancement',\n",
       " 'Envy',\n",
       " 'Excitement',\n",
       " 'Fear',\n",
       " 'Gratitude',\n",
       " 'Guilt',\n",
       " 'Horror',\n",
       " 'Interest',\n",
       " 'Joy',\n",
       " 'Love',\n",
       " 'Nostalgia',\n",
       " 'Pain',\n",
       " 'Pride',\n",
       " 'Realization',\n",
       " 'Relief',\n",
       " 'Romance',\n",
       " 'Sadness',\n",
       " 'Sarcasm',\n",
       " 'Satisfaction',\n",
       " 'Shame']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
